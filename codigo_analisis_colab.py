# -*- coding: utf-8 -*-
"""TAE_Taller_1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KcbKdnSB0RVi2jDvougTG5ONDwDzwMsY

#**Taller de Clustering no supervisado:** Información de instituciones de educación superior en U.S.
<center>

---
Técnicas de Aprendizaje Estadístico 


---
**Profesor:**

Juan David Ospina Arango

---
</center>

**Objetivos:**

*   Desarrollar un agrupamiento de instituciones de educación superior
*   Caracterizar cada grupo
*   Entender qué hace que un grupo sea una buena opción
"""

# Commented out IPython magic to ensure Python compatibility.
#Librerias necesarias 
import matplotlib
import numpy as np
import pandas as pd
import seaborn as sns
from tabulate import tabulate 
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn.cluster import KMeans
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.model_selection import train_test_split
from sklearn.cluster import AgglomerativeClustering
import plotly.express as px

# %matplotlib inline
from mpl_toolkits.mplot3d import Axes3D

"""## 1. Exploracion de los datos:


---

Importamos el conjunto de datos a través de una url desde nuestro repositorio de GitHub y lo guardamos en un dataframe que nombramos con la variable **college**
"""

#Para abrir desde colab
url = 'https://raw.githubusercontent.com/Jurregoz/tae_trabajo_1/main/CollegeScorecard.csv'
college = pd.read_csv(url)

"""Demos un vistazo al conjunto de datos"""

college.head()

"""Vemos que nuestro conjunto de datos cuenta con 7804 filas y 1725 columnas."""

college.info()

"""Dada la informacion anterior, el primer problema que nos encontramos es la gran cantidad de posibles variables, por eso pasamos a decidir cuales son las mas relevantes, para nuestro analisis.

## 2. Elección de variables y limpieza de datos


---

Luego de analizar adecuadamente el problema y el alcance del proyecto elegimos las variables que usaremos para llegar a una solución y las guardamos en un dataframe que llamaremos colleges. 

**Las Variables son:**


**INSTNM;** Nombre de la institucion

**CONTROL;** Identifica si la estructura de gobierno de la institución es:
 
 (1) Pública.

 (2) Privada **sin** ánimo de lucro.
 
 (3) Privada **con** ánimo de lucro.

**HCM2;** Es el tipo de HCM que indica problemas financieros o de cumplimiento federal más graves, HCM2 indica un sistema de supervision mayor dadas irregularidades en la institucion y HCM un nivel de monitoreo menor.

 (1) Tiene problemas (mayor monitorio)  
 (0) No tiene problemas, menor monitoreo del dinero.

**COSTT4_P;** Es el costo de asistencia tomado del programa academico más cursado durante el año.

**COSTT4_A;** Costo de asistencia anual general para todos los estudiantes.

**TUITIONFEE_IN;** Costo y tasas de matricula para estudiantes de dentro del estado

**TUITIONFEE_OUT;** Costo y tasas de matricula para estudiantes de fuera del estado.

**TUITIONFEE_PROG;** Costo y tasas de matricula general, tanto si es fuera o dentro del estado, esto son aquellas universidades que no cobran alguna diferencia por esto.

**STABBR;** Estado

Porcentaje de estudiantes dados los ingresos familiares.

**INC_PCT_LO,**  $($0-$30,000) 

**INC_PCT_M1,**  $($30,001-$48,000)

**INC_PCT_M2,** $($48,001-$75,000)

**INC_PCT_H1,** $($75,001-$110,000)

**INC_PCT_H2,**  $($$110,001+)
"""

colleges = college[['INSTNM', 'CONTROL', 'HCM2', 'COSTT4_P', 'COSTT4_A', 'TUITIONFEE_IN', 'TUITIONFEE_OUT', 'TUITIONFEE_PROG', 'STABBR', 'INC_PCT_LO', 'INC_PCT_M1', 'INC_PCT_M2', 'INC_PCT_H1', 'INC_PCT_H2']]

"""Demos un vistazo al conjunto de datos reducido"""

colleges.head()

"""De nuestras varibales seleccionadas vemos el panorama general, viendo los tipos de variables que son y la cantida de datos no nulos que tiene."""

colleges.info()

"""Vemos que las únicas variables que no tienen datos faltantes son, INSTNM, CONTROl, HCM2 y STBBR, pero cuando observamos las variables INC_PCT_* notamos que tienen una gran cantidad de filas únicamente con el valor *PrivacySupressed*. Asumiremos este valor como un valor faltante también."""

colleges = colleges.replace('PrivacySuppressed', np.nan)

colleges.info()

for row in colleges.columns[-5:]:
  print(colleges[row].value_counts(normalize=True, dropna=False))

"""Vemos que de estas columnas la cantidad de valor faltantes es menor al 50% del conjunto de datos. Asumimos estos valores faltantes como MAR (Missing At Random) y usaremos técnicas estadísticas para rellenarlos de manera consecuente con el resto de datos.

### 2.1. Manejo de datos faltantes

#### Variables INC_PCT_*
Las siguientes líneas rellenarán los datos faltantes de las variables INC_PCT_* usando multiple imputation y basándose en la variable CONTROL.
"""

imputedf = colleges[['CONTROL','INC_PCT_LO']]
 
traindf, testdf = train_test_split(imputedf, train_size=0.2)
 
# Create the IterativeImputer model to predict missing values
imp = IterativeImputer(max_iter=20, random_state=0)
 
# Fit the model to the the test dataset
imp.fit(imputedf)
 
# Transform the model on the entire dataset
colleges.INC_PCT_LO = pd.DataFrame(imp.transform(imputedf), columns=['CONTROL','INC_PCT_LO']).INC_PCT_LO

imputedf = colleges[['CONTROL','INC_PCT_M1']]
 
traindf, testdf = train_test_split(imputedf, train_size=0.2)
 
# Create the IterativeImputer model to predict missing values
imp = IterativeImputer(max_iter=20, random_state=0)
 
# Fit the model to the the test dataset
imp.fit(imputedf)
 
# Transform the model on the entire dataset
colleges.INC_PCT_M1 = pd.DataFrame(imp.transform(imputedf), columns=['CONTROL','INC_PCT_M1']).INC_PCT_M1

imputedf = colleges[['CONTROL','INC_PCT_M2']]
 
traindf, testdf = train_test_split(imputedf, train_size=0.2)
 
# Create the IterativeImputer model to predict missing values
imp = IterativeImputer(max_iter=20, random_state=0)
 
# Fit the model to the the test dataset
imp.fit(imputedf)
 
# Transform the model on the entire dataset
colleges.INC_PCT_M2 = pd.DataFrame(imp.transform(imputedf), columns=['CONTROL','INC_PCT_M2']).INC_PCT_M2

imputedf = colleges[['CONTROL','INC_PCT_H1']]
 
traindf, testdf = train_test_split(imputedf, train_size=0.2)
 
# Create the IterativeImputer model to predict missing values
imp = IterativeImputer(max_iter=20, random_state=0)
 
# Fit the model to the the test dataset
imp.fit(imputedf)
 
# Transform the model on the entire dataset
colleges.INC_PCT_H1 = pd.DataFrame(imp.transform(imputedf), columns=['CONTROL','INC_PCT_H1']).INC_PCT_H1

imputedf = colleges[['CONTROL','INC_PCT_H2']]
 
traindf, testdf = train_test_split(imputedf, train_size=0.2) 
 
# Create the IterativeImputer model to predict missing values
imp = IterativeImputer(max_iter=20, random_state=0)
 
# Fit the model to the the test dataset
imp.fit(imputedf)
 
# Transform the model on the entire dataset
colleges.INC_PCT_H2 = pd.DataFrame(imp.transform(imputedf), columns=['CONTROL','INC_PCT_H2']).INC_PCT_H2

"""Vemos que pudimos rellenar los datos faltantes de estas variables"""

colleges.info()

"""Ahora las columnas INC_PCT_* están completas, sin ningún dato faltante y podemos usarlas para nuestra solución.

#### Variables COSTT4_* y TUITIONFEE_*

Respecto a las variables COSTT4_* leyendo la documentación vemos que, ambas se complementan mutuamente, aunque parezca que tienen muchos datos nulos son complementarias, pues cada una se corresponde al costo de matrícula para un tipo específico de universidad entre dos posibles, es decir que, para el manejo de datos faltantes en estas dos variables, combinaremos en una sola columna los datos de ambas.
"""

# Usaremos una copia del conjunto de datos para evitar problemas
colleges_copy = colleges.copy()

#Seleccionamos los datos que estan en la columna COSTT4_P que no estan vacios
nuevos = colleges_copy['COSTT4_P'].dropna()
nuevos.head()

#Cambiamos los valores faltantes en COSTT4_A por los valores de COSTT4_P
colleges_copy.loc[colleges_copy["COSTT4_A"].isna(), 'COSTT4_A'] = nuevos #Asignamos a la columna la serie 
colleges_copy.info()

"""Finalmente vemos que la variable COSTT4_A ha quedado como la combinación de ella misma con la variable COSTT4_P y será la variable que usaremos para nuestro análisis.

Respecto a las variables TUITIONFEE_* dado que TUITIONFEE_IN y TUITIONFEE_OUT son variables que nos hablan del costo de matrícula para estudiantes del mismo estado y fuera el estado, podemos tomar el TUITIONFEE_PROG como un valor para llenar los elementos faltantes de TUITIONFEE_IN y TUITIONFEE_OUT tal como en el caso anterior, esto dado que TUITIONFEE_PROG corresponde a un costo general tanto para estudiantes fuera y de dentro del estado, recordemos que esta variable nos dice el costo anual de matrícula dado el costo del programa curricular más visto.
"""

#Seleccionamos los datos que estan en la columna TUITIONFEE_PROG que no estan vacios, los cuales usaremos 
#para llenar los datos faltantes en IN y OUT

nuevosT = colleges_copy['TUITIONFEE_PROG'].dropna()
nuevosT.head()

#Combiamos los valores faltantes en TUITIONFEE_IN por los valores de TUITIONFEE_PROG
colleges_copy.loc[colleges_copy["TUITIONFEE_IN"].isna(), 'TUITIONFEE_IN'] = nuevosT #Asignamos a la columna la serie 

#Combiamos los valores faltantes en TUITIONFEE_OUT por los valores de TUITIONFEE_PROG
colleges_copy.loc[colleges_copy["TUITIONFEE_OUT"].isna(), 'TUITIONFEE_OUT'] = nuevosT #Asignamos a la columna la serie 
colleges_copy.info()

"""Finalmente vemos que para las variables TUITIONFEE_IN y TUITIONFEE_OUT hemos reducido el numero de nulos

Antes de seguir con el analisis vamos a descartar aquellas variables que tenemos y no nos sirven a la hora de hacer el clustering
"""

#Primero descartamos aquellas variables que no nos aportan nada para el clustering que son el nombre y el codigo del estado.
#Ademas eliminamos las variables que ya no nos son utiles dado el manejo de datos faltantes anterior.
collegesA = colleges_copy.drop(['COSTT4_P', 'TUITIONFEE_PROG' ], axis= 1)
collegesA.info()

#Cambio en el nombre de las variables y lo guarderamos 
collegesA = collegesA.set_axis(['NOMBRE', 'CONTROL',  'TIPO_MONITOREO', 'COSTO_ANUAL_ESTUDIANTE', 'COSTO_MATRICULA', 'COSTO_MATRICULA_FUERA', 
                   'ESTADO', 'INGRESOS_FAMILIARES_LO', 'INGRESOS_FAMILIARES_M1', 'INGRESOS_FAMILIARES_M2', 'INGRESOS_FAMILIARES_H1', 
                   'INGRESOS_FAMILIARES_H2'], axis=1)
collegesA.info()

"""### 2.3 Analisis de variables resultantes

Vamos a darle cierta estructura a los datos de la variable ```ESTADO```. A cada estado le asignaremos un único número entero.
"""

#Antes de realizar cualquier análisis estadistico, se trabajará con los datos previamente mezclados, ademas codificaremos el estado.
from pandas.core.arrays.numeric import T
#Se crea el objeto de tipo encoded.
label_encoding = preprocessing.LabelEncoder()

#type de los datos para el modelo.
collegesA['ESTADO'] = label_encoding.fit_transform(collegesA['ESTADO'].astype(str))


collegesA = collegesA.sample(frac=1).reset_index(drop=True)
collegesA.head()

"""Veamos la matriz de correlación de las variables del conjunto de datos. Para esto no tendremos en cuenta las variables categóricas."""

#Haciendo la correlación de las variables con las variables significativas
colormap = plt.cm.viridis
plt.figure(figsize=(10,10))
plt.title('Matriz de correlacion')
sns.heatmap(collegesA.drop('NOMBRE', axis=1).astype(float).corr(),
           vmax=1.0,
           cmap=colormap,
           annot=True,
           linewidths=0.1,
           linecolor='white',
           square=True)

plt.show()

"""De la matriz de correlación vemos que hay algunas variables altamencte correlacionadas. Por esto vamos a eliminar al menos una de cada dos variables con una correlación mayor a 0.75 en valor absoluto."""

collegesA = collegesA.drop(['COSTO_ANUAL_ESTUDIANTE', 'COSTO_MATRICULA_FUERA',  'INGRESOS_FAMILIARES_LO', 
                                'INGRESOS_FAMILIARES_LO','INGRESOS_FAMILIARES_H1', 'INGRESOS_FAMILIARES_M2'], axis= 1)
collegesA.info()

"""Veamos una vez más la matriz de correlación con las nuevas variables."""

#Haciendo la correlación de las variables con las variables significativas
colormap = plt.cm.viridis
plt.figure(figsize=(10,10))
plt.title('Matriz de correlacion')
sns.heatmap(collegesA.drop('NOMBRE', axis=1).astype(float).corr(),
           vmax=1.0,
           cmap=colormap,
           annot=True,
           linewidths=0.1,
           linecolor='white',
           square=True)

plt.show()

"""Veamos un poco de la distribución de cada variable numérica y comparémoslas con otras variables mediante gráficas de dispersión."""

#Cómo están distribuidos los datos en el espacio
sns.pairplot(collegesA.dropna(),
            height=4, 
            vars=['COSTO_MATRICULA','INGRESOS_FAMILIARES_M1', 'INGRESOS_FAMILIARES_H2'],
            kind='scatter')
plt.show()

"""## 3. Cantidad Optima de clusters

---

#### Dendograma
"""

#Datos para el modelo
collegesA = collegesA.dropna()
collegesA.shape

#Note que ya no son 1000 registros.

"""Dado que las variables categoricas pueden generar problemas en el modelos hacemos pruebas para ver como estas los afectan"""

import scipy.cluster.hierarchy as shc
from matplotlib import pyplot
pyplot.figure(figsize=(15, 7))  
pyplot.title("Dendrograma SIN Tipo de Monitoreo y Control") 
collegesA_prueba = collegesA.drop(['TIPO_MONITOREO','CONTROL', 'NOMBRE'],axis=1) 
dend = shc.dendrogram(shc.linkage(collegesA_prueba, method='ward'),truncate_mode='level',p=3)

import scipy.cluster.hierarchy as shc
from matplotlib import pyplot
pyplot.figure(figsize=(15, 7))  
pyplot.title("Dendrograma CON Tipo de Monitoreo y Control") 
pyplot.axhline(y=200000)
pyplot.axhline(y=350000)
pyplot.axhline(y=600000)
dend = shc.dendrogram(shc.linkage(collegesA.drop('NOMBRE', axis=1), method='ward'),truncate_mode='level',p=3)

"""Se observa a través de los dendrogramas lo siguiente.
1. La variables *Tipo de Monitoreo y Control* no afecta la forma de agrupación de los datos.
2. Entre 2, 3 y 4 clusteres se obserta un número óptimo por medio de este gráfico.

#### Elbow Curve

`inertia_`: Suma de las distancias cuadráticas de las muestras al centro del cluster mas cercano.

Este método usa la inercia como una medida de la variación intra-cluster e intenta minimizarla. Se debe tener en cuenta que la inercia óptima (mínima) sería cero, para el caso donde cada observación es su propio cluster; pero como esto no es de mucha utilidad, el método del codo escoge como óptimo aquel valor del número de clusters a partir del cual añadir mas clusters solo consigue una mejora mínima de la inercia.
"""

colleges_3D = np.array(collegesA.drop('NOMBRE', axis=1))
#[['attack','defense', 'speed']]
colleges_labels = np.array(college['OPEID'])
print(colleges_3D.shape)

# se define la cantidad de clusters con los que se quiere probar
maxClusters = 20

def elbow_curve(data, maxClusters = 15):

  # rango de valores del parámetro a optimizar (cantidad de clusters)
  maxClusters = range(1, maxClusters + 1)
  inertias = []

  # se ejecuta el modelo para el rango de clusters y se guarda la inercia
  # respectiva obtenida para cada valor
  for k in maxClusters:
    kmeanModel = KMeans(n_clusters = k)
    kmeanModel.fit(colleges_3D)
    inertias.append(kmeanModel.inertia_)

  # Grafico de los resultados obtenidos para cada valor del rango
  print("Valores: ",inertias)
  plt.figure(figsize=(10, 8))
  plt.plot(maxClusters, inertias, 'bx-')
  plt.xlabel('k')
  plt.ylabel('Inertia')
  plt.title('The Elbow Method showing the optimal k')
  plt.show()

elbow_curve(colleges_3D, maxClusters = 10)

"""Vemos que la forma del gráfico nos sugiere que el número adecuado de grupos es de 3 o 4 como máximo.

#### Estadístico de Gap
El objetivo de este método es definir un procedimiento estadístico para formalizar la heurística de la curva de codo. De forma muy simplificada, el estadístico de gap compara, para diferentes valores de k, la variación total intracluster observada frente al valor esperado acorde a una distribución uniforme de referencia (datos de referencia).

El valor a elegir será el k mas pequeño tal que en k+1 el gráfico caiga (no necesariamente es el máximo absoluto de la curva de gap).
"""

# nrefs es la cantidad de datos ("datasets") de referencia contra los que se va a comparar
def optimalK(data, nrefs=3, maxClusters=15):

    gaps = np.zeros((len(range(1, maxClusters+1)),))
    resultsdf = pd.DataFrame({'clusterCount':[], 'gap':[]})
    for gap_index, k in enumerate(range(1, maxClusters+1)):

        # guardara los resultados de dispersión de cada distribución simulada
        refDisps = np.zeros(nrefs)

        # Genera las muestras aleatorias indicadas con nrefs y ejecuta k-means
        # en cada bucle obteniendo los resultados de dispersión (inercia)
        # para cada conjunto generado.
        for i in range(nrefs):
            
            # Crea nuevo conjunto aleatorio de referencia
            # Se puede usar una semilla para tener reproducibilidad
            np.random.seed(0)
            randomReference = np.random.random_sample(size=data.shape)
            
            # se ajusta el modelo al conjunto de referencia
            km = KMeans(k)
            km.fit(randomReference)
            # se guarda la dispersión obtenida
            refDisp = km.inertia_
            refDisps[i] = refDisp

        # Se ajusta el modelo a los datos originales y se guarda su inercia
        km = KMeans(k)
        km.fit(data)
        
        origDisp = km.inertia_

        # Calcula el estadístico de gap para k clusters usando el promedio de
        # las dispersiones de los datos simulados y la dispersión de los datos originales.
        gap = np.log(np.mean(refDisps)) - np.log(origDisp)

        # Guarda el estadístico de gap obtenido en este bucle.
        gaps[gap_index] = gap
        
        resultsdf = resultsdf.append({'clusterCount':k, 'gap':gap}, ignore_index=True)

    # Selecciona el "primer máximo" de los estadísticos obtenidos y devuelve 
    # su respectivo número de clusters    
    for i in range(0, len(gaps)-1):
      if gaps[i+1] <= gaps[i]:
        return (i+1, resultsdf)
    return (len(gaps), resultsdf)
    #return (gaps.argmax() + 1, resultsdf)  # Plus 1 because index of 0 means 1 cluster is optimal, index 2 = 3 clusters are optimal

k, gapdf = optimalK(colleges_3D, nrefs=5, maxClusters=10)
print('La cantidad óptima de clusters es: ', k)

plt.figure(figsize=(16,8))
plt.plot(gapdf['clusterCount'], gapdf['gap'], linestyle='--', marker='o', color='b');
plt.xlabel('K');
plt.ylabel('Gap Statistic');
plt.title('Gap Statistic vs. K');

"""#### Coeficiente de Silueta"""

#Haciendo prueba las variable TIPO DE MONITOREO generan problemas para este analisis por eso los eliminamos 
colleges_3D = collegesA.drop(['TIPO_MONITOREO','CONTROL'], axis =1).copy()
#'CONTROL', 'ESTADO'

colleges_3DA = np.array(colleges_3D.drop('NOMBRE', axis=1))

colleges_labels = np.array(college['OPEID'])
print(colleges_3D.shape)

from sklearn.metrics import silhouette_samples, silhouette_score
import matplotlib.cm as cm

colors_k_means = ['cyan','purple','orange']
range_n_clusters = [2, 3, 4, 5, 6]
X = colleges_3DA

for n_clusters in range_n_clusters:
    # Create a subplot with 1 row and 2 columns
    fig, (ax1, ax2) = plt.subplots(1, 2)
    fig.set_size_inches(19, 4)

    # The 1st subplot is the silhouette plot
    # The silhouette coefficient can range from -1, 1 but in this example all
    # lie within [-0.1, 1]
    ax1.set_xlim([-0.1, 1])
    # The (n_clusters+1)*10 is for inserting blank space between silhouette
    # plots of individual clusters, to demarcate them clearly.
    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])

    # Initialize the clusterer with n_clusters value and a random generator
    # seed of 10 for reproducibility.
    clusterer = KMeans(n_clusters=n_clusters, random_state=10)
    cluster_labels = clusterer.fit_predict(X)

    # The silhouette_score gives the average value for all the samples.
    # This gives a perspective into the density and separation of the formed
    # clusters
    silhouette_avg = silhouette_score(X, cluster_labels)
    print("For n_clusters =", n_clusters,
          "The average silhouette_score is :", silhouette_avg)

    # Compute the silhouette scores for each sample
    sample_silhouette_values = silhouette_samples(X, cluster_labels)

    y_lower = 10
    for i in range(n_clusters):
        # Aggregate the silhouette scores for samples belonging to
        # cluster i, and sort them
        ith_cluster_silhouette_values = \
            sample_silhouette_values[cluster_labels == i]

        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = plt.cm.nipy_spectral(float(i) / n_clusters)
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)

        # Label the silhouette plots with their cluster numbers at the middle
        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

        # Compute the new y_lower for next plot
        y_lower = y_upper + 10  # 10 for the 0 samples

    ax1.set_title("The silhouette plot for the various clusters.")
    ax1.set_xlabel("The silhouette coefficient values")
    ax1.set_ylabel("Cluster label")

    # The vertical line for average silhouette score of all the values
    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

    ax1.set_yticks([])  # Clear the yaxis labels / ticks
    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

    # 2nd Plot showing the actual clusters formed
    colors = plt.cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)
    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,
                c=colors, edgecolor='k')

    # Labeling the clusters
    centers = clusterer.cluster_centers_
    # Draw white circles at cluster centers
    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',
                c="white", alpha=1, s=200, edgecolor='k')

    for i, c in enumerate(centers):
        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,
                    s=50, edgecolor='k')

    ax2.set_title("The visualization of the clustered data.")
    ax2.set_xlabel("Feature space for the 1st feature")
    ax2.set_ylabel("Feature space for the 2nd feature")

    plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
                  "with n_clusters = %d" % n_clusters),
                 fontsize=14, fontweight='bold')

plt.show()

"""Del los cuatro métodos utilizados vemos que dos nos sugieren utilizar 3 grupos. De esta manera consideramos que este es el número óptimo de grupos y haremos el análisis con este número.

## 4. Clustering - Jerárquico

---

Ahora vamos a implementar un modelo de clustering sobre los datos que tenemos y despues vamos a caracterizar cada uno de los grupos que nos arroje este.
"""

colleges_3D = collegesA.copy()
colleges_3D.info()

#Normalizar, por buenas practicas, antes de aplicar el modelo.
def minmax_norm(df):
    return (df - df.min()) / ( df.max() - df.min())

#Para hacer el clustering usaremos solo las variables 'COSTO_MATRICULA','INGRESOS_FAMILIARES_M1', 'INGRESOS_FAMILIARES_H2'
collegesA_Norma = colleges_3D.drop(['NOMBRE', 'TIPO_MONITOREO','ESTADO', 'CONTROL'], axis =1).copy()       #Copia del dataframe para no alterarlo, Usamos colleges 3D que no contiene a las variables 
                                            #categoricas, CONTROL y TIPO DE MONITOREO, para evitar posibles errores.
                                            
collegesA_Norma = minmax_norm(collegesA_Norma)
collegesA_Norma.head()

#Crear el modelo con la metrica euclidiana y vinculación minimizando la distancia.
# affinity=" " para otros criterios de similitud diferentes al euclidiano que viene por defecto
model=AgglomerativeClustering(n_clusters=3, linkage='ward')

#Aplicar el modelo
data_fit_3 = model.fit(collegesA_Norma)
lab_3c = data_fit_3.labels_

#Se asignará cada label a cada fila correspondiente
#crimen_df_Data_Copy['Labels_3Clusters']=lab_3c     #No se deben realizar cambios a los datos asignados para el modelo
colleges_3D['Labels_3Clusters']=lab_3c
colleges_3D.tail()
#Note que no se le aplicó 'Labels_3Cluster' a crimen_df_Data_Copy (Por ser un dataframe normalizado).
#crimen_df_Data.tail()

#¿Como visualizar los clusters?
X=np.array(colleges_3D.drop(['NOMBRE', 'CONTROL','ESTADO','TIPO_MONITOREO'],axis=1))
fig = plt.figure(figsize=(10, 8))
ax = Axes3D(fig)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=lab_3c, s=200,
           cmap=matplotlib.colors.ListedColormap(['cyan','purple','orange']), alpha=0.5)
plt.show()

#Otra forma de visualizar los datos en 3D
#Cómo están distribuidos los datos en el espacio
fig = px.scatter_3d(colleges_3D.dropna(), x="INGRESOS_FAMILIARES_M1", y="INGRESOS_FAMILIARES_H2", z="COSTO_MATRICULA",color="Labels_3Clusters")
fig.show()

#Otra forma de visualizar los datos en 3D
#Cómo están distribuidos los datos en el espacio
fig = px.scatter_3d(colleges_3D.dropna(), x="CONTROL", y="INGRESOS_FAMILIARES_M1", z="COSTO_MATRICULA",color="Labels_3Clusters")
fig.show()

#Otra forma de visualizar los datos en 3D
#Cómo están distribuidos los datos en el espacio
fig = px.scatter_3d(colleges_3D.dropna(), x="CONTROL", y="INGRESOS_FAMILIARES_H2", z="COSTO_MATRICULA",color="Labels_3Clusters")
fig.show()

#Otra forma de visualizar los datos en 3D
#Cómo están distribuidos los datos en el espacio clasificados por el tipo de CONTROL (universidad privada, publica, privada sin fines de lucro)
fig = px.scatter_3d(colleges_3D.dropna(), x="INGRESOS_FAMILIARES_M1", y="INGRESOS_FAMILIARES_H2", z="COSTO_MATRICULA",color="CONTROL")
fig.show()

"""## 5. Caracterización de los grupos

---

Veamos un poco más sobre el comportamiento de los grupos. Hagamos un pequeño análisis descriptivo sobre cada grupo.

### GRUPO 0:
"""

#Se separan por labels por medio de una máscara.
#Grupo 0
print("GRUPO 0:")
is_G=colleges_3D.loc[:, 'Labels_3Clusters']==0
C3_G=colleges_3D[is_G]
print(C3_G.head())
print(C3_G.shape)

sns.pairplot(C3_G,
            height=3, 
            vars=['COSTO_MATRICULA','INGRESOS_FAMILIARES_M1', 'INGRESOS_FAMILIARES_H2'],
            kind='scatter')
plt.show()

grupo_0 = colleges_3D[colleges_3D.Labels_3Clusters == 0]
grupo_0.head()

"""Vamos a darle formato de string a las variables categóricas para garantizar un análisis más limpio"""

grupo_0 = grupo_0.astype({'CONTROL': 'object', 'TIPO_MONITOREO': 'object', 'ESTADO': 'object' })
grupo_0.info()

grupo_0.describe()

"""### GRUPO 1:"""

#Se separan por labels por medio de una máscara.
#Grupo 0
print("GRUPO 1:")
is_G=colleges_3D.loc[:, 'Labels_3Clusters']==1
C3_G=colleges_3D[is_G]
print(C3_G.head())
print(C3_G.shape)

sns.pairplot(C3_G,
            height=3, 
            vars=['COSTO_MATRICULA','INGRESOS_FAMILIARES_M1', 'INGRESOS_FAMILIARES_H2'],
            kind='scatter')
plt.show()

grupo_1 = colleges_3D[colleges_3D.Labels_3Clusters ==1]
grupo_1 = grupo_1.astype({'CONTROL': 'object', 'TIPO_MONITOREO': 'object', 'ESTADO': 'object' })
grupo_1.describe()

"""### GRUPO 2:"""

#Se separan por labels por medio de una máscara.
#Grupo 2
print("GRUPO 2:")
is_G=colleges_3D.loc[:, 'Labels_3Clusters']==2
C3_G=colleges_3D[is_G]
print(C3_G.head())
print(C3_G.shape)

sns.pairplot(C3_G,
            height=3, 
            vars=['COSTO_MATRICULA','INGRESOS_FAMILIARES_M1', 'INGRESOS_FAMILIARES_H2'],
            kind='scatter')
plt.show()

grupo_2 = colleges_3D[colleges_3D.Labels_3Clusters ==2]
grupo_2 = grupo_2.astype({'CONTROL': 'object', 'TIPO_MONITOREO': 'object', 'ESTADO': 'object' })
grupo_2.describe()

"""Finalmente descargamos los datos resultantes"""

from google.colab import files

colleges_3D.to_csv('colleges_agrupados.csv') 
grupo_0.to_csv('grupo_0.csv')
grupo_1.to_csv('grupo_1.csv')
grupo_2.to_csv('grupo_2.csv')

files.download('colleges_agrupados.csv')
files.download('grupo_1.csv')
files.download('grupo_2.csv')
files.download('grupo_0.csv')

"""## 6. Conclusiones

---

### 1

Recordemos el significado de los siguientes elementos:

Porcentaje de estudiantes dados los ingresos familiares.

**INC_PCT_M1,**  $($30,001-$48,000)

**INC_PCT_H2,**  $($$110,001+)

Grupo 0 (Azul): En este grupo se encuentran universidades con un costo promedio de matricula de (16000 dolares), pero que puede llegar a variar mucho. En este grupo se encuentra un mayor porcentaje de estudiantes M1 en comparacion con los de H2.

Grupo 1 (Rosado): En este grupo se encuentran universidades con un costo promedio de matricula de (28000 dolares), siendo este el grupo con el mayor costo promedio de matricula y el mayor porcentaje de estudiantes H2.

Grupo 2 (Amarillo): En este grupo se encuentran universidades con un costo promedio de matricula de (4000 dolares), siendo este el grupo con el menor costo promedio de matricula y el mayor porcentajes de estudiantes M1.

Pese a lo anterior, notemos que la universidad de mayor costo no está en el grupo 1 sino en el grupo 0. Aún así, la desviación estandar y el rango intercuartil del grupo 0 son relativamente bajos, lo que sugiere que la distribución del costo en este grupo no está muy dispersa y el máximo es solo un valor atípico; veamos exactamente qué universidad es.
"""

grupo_0[grupo_0.COSTO_MATRICULA == grupo_0.COSTO_MATRICULA.max()]

"""- Dado todo el analisis anterior podriamos resumir los 3 grupos en los siguiente:

**Grupo 0: universides de costo medio y con estudiantes principalmente de clase media**

**Grupo 1: Universidades de costo alto y estudiantes principalmente de clase alta**

**Grupo 2: Universidades de costo bajo y estudiantes principalmente de clase media**

### 2

Tambien encontramos que existe una relacion entre los grupos que encontramos, con la variable *CONTROL*. Recordemos que esta variable nos identifica si la estructura de gobierno de la institución es:
 
 (1) Pública.

 (2) Privada **sin** ánimo de lucro.
 
 (3) Privada **con** ánimo de lucro.

Vemos que nuestro grupo 2 conformado por las universidades con menor costo promedio de matricula esta muy relacionado con aquellas universidades que son publicas.
Vemos que nuestro grupo 1 conformado por las universidades con mayor costo promedio de matricula esta muy relacionado con aquellas universidades que son Privadas sin animo de lucro.
Finalmente tambien vemos que para nuestro grupo 0 con los costos medios se relaciona con aquellas universidades que son privadas con animo de lucro.
"""

#Otra forma de visualizar los datos en 3D
#Cómo están distribuidos los datos en el espacio, dado los grupos encontrados
fig = px.scatter_3d(colleges_3D.dropna(), x="INGRESOS_FAMILIARES_M1", y="INGRESOS_FAMILIARES_H2", z="COSTO_MATRICULA",color="Labels_3Clusters")
fig.show()

#Otra forma de visualizar los datos en 3D
#Cómo están distribuidos los datos en el espacio, dado el tipo de control
fig = px.scatter_3d(colleges_3D.dropna(), x="INGRESOS_FAMILIARES_M1", y="INGRESOS_FAMILIARES_H2", z="COSTO_MATRICULA",color="CONTROL")
fig.show()