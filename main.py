# -*- coding: utf-8 -*-
"""TAE_Taller_1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KcbKdnSB0RVi2jDvougTG5ONDwDzwMsY

#**Taller Clustering no supervisado:** Informacion de intituciones de educacion en U.S.
<center>

---
Tecnicas de Aprendizaje Estadistico 


---
**Profesor:**

Juan David Ospina Arango

---
</center>

**Objetivos:**

*   Desarrollar un agrupamiento de instituciones de educación superior
*   Caracterizar cada grupo
*   Entender qué hace que un grupo sea una buena opción
"""

# Commented out IPython magic to ensure Python compatibility.
#Librerias necesarias 
import matplotlib
import numpy as np
import pandas as pd
import seaborn as sns
from tabulate import tabulate 
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn.cluster import KMeans
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.model_selection import train_test_split
from sklearn.cluster import AgglomerativeClustering
import plotly.express as px

# %matplotlib inline
from mpl_toolkits.mplot3d import Axes3D

"""## 1. Exploracion de los datos:


---

Importamos el conjunto de datos a través de una url desde nuestro repositorio de GitHub y lo guardamos en un dataframe que nombramos con la variable **college**
"""

#Para abrir desde colab
url = 'https://raw.githubusercontent.com/Jurregoz/tae_trabajo_1/main/CollegeScorecard.csv'
college = pd.read_csv(url)

"""Demos un vistazo al conjunto de datos"""

college.head()

"""Vemos que nuestro conjunto de datos cuenta con 7804 filas y 1725 columnas o tambien llamadas posibles variables."""

college.info()

"""Dada la informacion anterior, el primer problema que nos encontramos es la gran cantidad de posibles variables, por eso pasamos a decidir cuales son las mas relevantes, para nuestro analisis.

## 2. Elección de variables y limpieza de datos


---

Luego de analizar adecuadamente el problema y el alcance del proyecto elegimos las primeras posibles variables que usaremos para llegar a una solución y las guardamos en un dataframe que llamaremos colleges. 

**Las Variables son:**


**INSTNM;** Nombre de la institucion

**CONTROL;** Identifica si la estructura de gobierno de la institución es:
 
 pública (1), 
 
 privada sin ánimo de lucro (2)

 privada con ánimo de lucro (3).

**HCM2;** Es el tipo de HCM que indica problemas financieros o de cumplimiento federal más graves, HCM2 indica un sistema de supervision mayor dadas irregularidades en la institucion y HCM un nivel de monitoreo menor.

 (1) tiene problemas (mayor monitorio)  
 (0) No tiene problemas, menor monitoreo del dinero.

**COSTT4_P;** Es el costo de asistencia tomado del programa academico más cursado durante el año.

**COSTT4_A;** Costo de asistencia anual general para todos los estudiantes.

**TUITIONFEE_IN;** Costo y tasas de matricula para estudiantes de dentro del estado

**TUITIONFEE_OUT;** Costo y tasas de matricula para estudiantes de fuera del estado.

**TUITIONFEE_PROG;** Costo y tasas de matricula general, tanto si es fuera o dentro del estado, esto son aquellas universidades que no cobran alguna diferencia por esto.

**STABBR;** Estado

Porcentaje de estudiantes dados los ingresos familiares.

**INC_PCT_LO,**  $($0-$30,000) 

**INC_PCT_M1,**  $($30,001-$48,000)

**INC_PCT_M2,** $($48,001-$75,000)

**INC_PCT_H1,** $($75,001-$110,000)

**INC_PCT_H2,**  $($$110,001+)
"""

colleges = college[['INSTNM', 'CONTROL', 'HCM2', 'COSTT4_P', 'COSTT4_A', 'TUITIONFEE_IN', 'TUITIONFEE_OUT', 'TUITIONFEE_PROG', 'STABBR', 'INC_PCT_LO', 'INC_PCT_M1', 'INC_PCT_M2', 'INC_PCT_H1', 'INC_PCT_H2']]

"""Demos un vistazo al conjunto de datos reducido"""

colleges.head()

"""De nuestras varibales seleccionadas vemos el panorama general, viendo los tipos de variables que son y la cantida de datos no nulos que tiene."""

colleges.info()

"""Vemos que las únicas variables que no tienen datos faltantes son, INSTNM, CONTROl, HCM2 y STBBR, pero cuando observamos las variables INC_PCT_* notamos que tienen una gran cantidad de filas únicamente con el valor *PrivacySupressed*. Asumiremos este valor como un valor faltante también."""

colleges = colleges.replace('PrivacySuppressed', np.nan)

colleges.info()

for row in colleges.columns[-5:]:
  print(colleges[row].value_counts(normalize=True, dropna=False))

"""Vemos que de estas columnas la cantidad de valor faltantes es menor al 50% del conjunto de datos. Asumimos estos valores faltantes como MAR (Missing At Random) y usaremos técnicas estadísticas para rellenarlos de manera consecuente con el resto de datos.

### 2.1. Manejo de datos faltantes

Las siguientes líneas rellenarán los datos faltantes de las variables INC_PCT_* usando multiple imputation y basándose en la variable CONTROL.

#### Variables INC_PCT_*

Las siguientes líneas rellenarán los datos faltantes de las variables INC_PCT_* usando multiple imputation y basándose en la variable CONTROL.
"""

imputedf = colleges[['CONTROL','INC_PCT_LO']]
 
traindf, testdf = train_test_split(imputedf, train_size=0.2)
 
# Create the IterativeImputer model to predict missing values
imp = IterativeImputer(max_iter=20, random_state=0)
 
# Fit the model to the the test dataset
imp.fit(imputedf)
 
# Transform the model on the entire dataset
colleges.INC_PCT_LO = pd.DataFrame(imp.transform(imputedf), columns=['CONTROL','INC_PCT_LO']).INC_PCT_LO

imputedf = colleges[['CONTROL','INC_PCT_M1']]
 
traindf, testdf = train_test_split(imputedf, train_size=0.2)
 
# Create the IterativeImputer model to predict missing values
imp = IterativeImputer(max_iter=20, random_state=0)
 
# Fit the model to the the test dataset
imp.fit(imputedf)
 
# Transform the model on the entire dataset
colleges.INC_PCT_M1 = pd.DataFrame(imp.transform(imputedf), columns=['CONTROL','INC_PCT_M1']).INC_PCT_M1

imputedf = colleges[['CONTROL','INC_PCT_M2']]
 
traindf, testdf = train_test_split(imputedf, train_size=0.2)
 
# Create the IterativeImputer model to predict missing values
imp = IterativeImputer(max_iter=20, random_state=0)
 
# Fit the model to the the test dataset
imp.fit(imputedf)
 
# Transform the model on the entire dataset
colleges.INC_PCT_M2 = pd.DataFrame(imp.transform(imputedf), columns=['CONTROL','INC_PCT_M2']).INC_PCT_M2

imputedf = colleges[['CONTROL','INC_PCT_H1']]
 
traindf, testdf = train_test_split(imputedf, train_size=0.2)
 
# Create the IterativeImputer model to predict missing values
imp = IterativeImputer(max_iter=20, random_state=0)
 
# Fit the model to the the test dataset
imp.fit(imputedf)
 
# Transform the model on the entire dataset
colleges.INC_PCT_H1 = pd.DataFrame(imp.transform(imputedf), columns=['CONTROL','INC_PCT_H1']).INC_PCT_H1

imputedf = colleges[['CONTROL','INC_PCT_H2']]
 
traindf, testdf = train_test_split(imputedf, train_size=0.2) 
 
# Create the IterativeImputer model to predict missing values
imp = IterativeImputer(max_iter=20, random_state=0)
 
# Fit the model to the the test dataset
imp.fit(imputedf)
 
# Transform the model on the entire dataset
colleges.INC_PCT_H2 = pd.DataFrame(imp.transform(imputedf), columns=['CONTROL','INC_PCT_H2']).INC_PCT_H2

"""Vemos que pudimos rellenar los datos faltantes de estas variables"""

colleges.info()

"""Ahora las columnas INC_PCT_* están completas, sin ningún dato faltante y podemos usarlas para nuestra solución.

#### Variables COSTT4_* y TUITIONFEE_*

##### Solucion A

Respecto a las variables COSTT4_* leyendo la documentación vemos que aunque parezca que tienen muchos datos nulos, son complementarias, pues cada una se corresponde al costo de matrícula para un tipo específico de universidad entre dos posibles, es decir que para el manejo de datos faltantes en estas dos variables combinaremos en una de las columnas los datos de ambas
"""

# Una copia para la solucion A
colleges_copy = colleges.copy()

#Seleccionamos los datos que estan en la columna COSTT4_P que no estan vacios
nuevos = colleges_copy['COSTT4_P'].dropna()
nuevos.head()

#Cambiamos los valores faltantes en COSTT4_A por los valores de COSTT4_P
colleges_copy.loc[colleges_copy["COSTT4_A"].isna(), 'COSTT4_A'] = nuevos #Asignamos a la columna la serie 
colleges_copy.info()

"""Finalmente vemos que la variable COSTT4_A ha quedado como la union de ambos datos no faltantes y será la que usaremos.

Respecto a las variables TUITIONFEE_* dado que TUITIONFEE_IN y TUITIONFEE_OUT son variables que nos hablan del costo de matricula para estudiantes del mismo estado y fuera el estado, podemos tomar el TUITIONFEE_PROG como un valor para llenar los elementos faltantes de TUITIONFEE_IN tal como en el caso anterior.
"""

#Seleccionamos los datos que estan en la columna TUITIONFEE_PROG que no estan vacios, los cuales usaremos 
#para llenar los datos faltantes en IN y OUT

nuevosT = colleges_copy['TUITIONFEE_PROG'].dropna()
nuevosT.head()

#Combiamos los valores faltantes en TUITIONFEE_IN por los valores de TUITIONFEE_PROG
colleges_copy.loc[colleges_copy["TUITIONFEE_IN"].isna(), 'TUITIONFEE_IN'] = nuevosT #Asignamos a la columna la serie 

#Combiamos los valores faltantes en TUITIONFEE_OUT por los valores de TUITIONFEE_PROG
colleges_copy.loc[colleges_copy["TUITIONFEE_OUT"].isna(), 'TUITIONFEE_OUT'] = nuevosT #Asignamos a la columna la serie 
colleges_copy.info()

"""Finalmente vemos que para las variables TUITIONFEE_IN y TUITIONFEE_OUT hemos reducido el numero de nulos

Antes de seguir con el analisis vamos a descartar aquellas variables que tenemos y no nos sirven a la hora de hacer el clustering
"""

#Primero descartamos aquellas variables que no nos aportan nada para el clustering que son el nombre y el codigo del estado.
#Ademas eliminamos las variables que ya no nos son utiles dado el manejo de datos faltantes anterior.
collegesA = colleges_copy.drop(['COSTT4_P', 'TUITIONFEE_PROG' ], axis= 1)
collegesA.info()

#Cambio en el nombre de las variables y lo guarderamos 
collegesA = collegesA.set_axis(['NOMBRE', 'CONTROL',  'TIPO_MONITOREO', 'COSTO_ANUAL_ESTUDIANTE', 'COSTO_MATRICULA', 'COSTO_MATRICULA_FUERA', 
                   'ESTADO', 'INGRESOS_FAMILIARES_LO', 'INGRESOS_FAMILIARES_M1', 'INGRESOS_FAMILIARES_M2', 'INGRESOS_FAMILIARES_H1', 
                   'INGRESOS_FAMILIARES_H2'], axis=1)
collegesA.info()

"""##### Solucion B

Respecto a las variables COSTT4_* leyendo la documentación vemos que aunque parezca que tienen muchos datos nulos, son complementarias, pues cada una se corresponde al costo de matrícula para un tipo específico de universidad entre dos posibles, es decir que para el manejo de datos faltantes en estas dos variables solo consideramos aquellos que falten en ambas. 

Asumiremos también estos datos faltantes como MAR y vamos a eliminar los registros donde todos ambos campos son faltantes.
"""

collegesB = colleges.copy()

collegesB.dropna(axis=0, subset=['COSTT4_P', 'COSTT4_A'], how='all', inplace=True)
collegesB.info()

"""Hagamos lo mismo para las columnas ```TUITIONFEE_*```


"""

collegesB.dropna(axis=0, subset=['TUITIONFEE_IN', 'TUITIONFEE_OUT', 'TUITIONFEE_PROG'], how='all', inplace=True)

collegesB.info()

"""De esta manera completamos la preparación de los datos.

### 2.3 Analisis de variables resultantes

#### Solucion A

Vamos a darle cierta estructura a los datos de la variable ```ESTADO```. A cada estado le asignaremos un único número entero.
"""

#Antes de realizar cualquier análisis estadistico, se trabajará con los datos previamente mezclados, ademas codificaremos el estado.
from pandas.core.arrays.numeric import T
#Se crea el objeto de tipo encoded.
label_encoding = preprocessing.LabelEncoder()

#type de los datos para el modelo.
collegesA['ESTADO'] = label_encoding.fit_transform(collegesA['ESTADO'].astype(str))


collegesA = collegesA.sample(frac=1).reset_index(drop=True)
collegesA.head()

"""Veamos la matriz de correlación de las variables del conjunto de datos. Para esto no tendremos en cuenta las variables categóricas."""

#Haciendo la correlación de las variables con las variables significativas
colormap = plt.cm.viridis
plt.figure(figsize=(10,10))
plt.title('Matriz de correlacion')
sns.heatmap(collegesA.drop('NOMBRE', axis=1).astype(float).corr(),
           vmax=1.0,
           cmap=colormap,
           annot=True,
           linewidths=0.1,
           linecolor='white',
           square=True)

plt.show()

"""De la amtriz de correlación vemos que hay algunas variables altamencte correlacionadas. Por esto vamos a eliminar al menos una de cada dos variables con una correlación mayor a 0.75 en valor absoluto."""

collegesA = collegesA.drop(['COSTO_ANUAL_ESTUDIANTE', 'COSTO_MATRICULA_FUERA',  'INGRESOS_FAMILIARES_LO', 
                                'INGRESOS_FAMILIARES_LO','INGRESOS_FAMILIARES_H1', 'INGRESOS_FAMILIARES_M2'], axis= 1)
collegesA.info()

"""Veamos una vez más la matriz de correlación con las nuevas variables."""

#Haciendo la correlación de las variables con las variables significativas
colormap = plt.cm.viridis
plt.figure(figsize=(10,10))
plt.title('Matriz de correlacion')
sns.heatmap(collegesA.drop('NOMBRE', axis=1).astype(float).corr(),
           vmax=1.0,
           cmap=colormap,
           annot=True,
           linewidths=0.1,
           linecolor='white',
           square=True)

plt.show()

"""Veamos un poco de la distribución de cada variable numérica y comparémoslas con otras variables mediante gráficas de dispersión."""

#Cómo están distribuidos los datos en el espacio
sns.pairplot(collegesA.dropna(),
            height=4, 
            vars=['COSTO_MATRICULA','INGRESOS_FAMILIARES_M1', 'INGRESOS_FAMILIARES_H2'],
            kind='scatter')
plt.show()

"""#### Solucion B"""

#Haciendo la correlación de las variables con las variables significativas
colormap = plt.cm.viridis
plt.figure(figsize=(10,10))
plt.title('Matriz de correlacion')
sns.heatmap(collegesB.drop(['STABBR','INSTNM'],axis=1).astype(float).corr(),
           vmax=1.0,
           cmap=colormap,
           annot=True,
           linewidths=0.1,
           linecolor='white',
           square=True)

plt.show()

"""Vemos que hay algunas celdad en blanco. Esto es debido a que ninguna universidad tiene simultáneamente datos tanto en COSTT4_A como en COSTT4_P por cómo están clasificadas las universidades. Para poder llevar a cabo el análisis divideremos el conjunto de datos en otros dos conjuntows más pequeños correspondientes a ***Program Year Institutions*** y ***Academic Year Institutions***

Las universidades clasificadas como program year Institutions las guardaremos en ```  program_year_colleges ```
"""

program_year_colleges = collegesB[college.COSTT4_A.isna()].dropna(axis=1)

program_year_colleges.info()

"""Las universidades clasificadas como academic year Institutions las guardaremos en ```  academic_year_colleges ```

"""

academic_year_colleges = collegesB[college.COSTT4_P.isna()].dropna(axis=1)

academic_year_colleges.info()

"""Veamos de nuevo las matrices de correlación asociadas a cada uno de los conjuntos de datos."""

#Antes de realizar cualquier análisis estadistico, se trabajará con los datos previamente mezclados.
program_year_colleges = program_year_colleges.sample(frac=1).reset_index(drop=True)
academic_year_colleges = academic_year_colleges.sample(frac=1).reset_index(drop=True)

colormap = plt.cm.viridis
plt.figure(figsize=(10,10))
plt.title('Matriz de correlacion de program_year_colleges')
sns.heatmap(program_year_colleges.drop(['STABBR','INSTNM'], axis = 1).astype(float).corr(),
           vmax=1.0,
            vmin=-1,
           cmap=colormap,
           annot=True,
           linewidths=0.1,
           linecolor='white',
           square=True)

plt.show()

colormap = plt.cm.viridis
plt.figure(figsize=(10,10))
plt.title('Matriz de correlacion de academic_year_colleges')
sns.heatmap(academic_year_colleges.drop(['STABBR','INSTNM'], axis = 1).astype(float).corr(),
           vmax=1.0,
            vmin=-1,
           cmap=colormap,
           annot=True,
           linewidths=0.1,
           linecolor='white',
           square=True)

plt.show()

"""Tal como vemos en ``` academic_year_institutions ``` existen variables que están altamente correlacionadas en este pero que en ``` program_year_institutions ``` no lo están. Esto nos sugiere aún más que es buena idea realizar el análisis en ambos conjuntos de datos de forma separada. Eliminaremos variables que estén correlacionadas en más de un 0.75 en valor absoluto con alguna otra; esto significa que solo eliminaremos variables de ``` academic_year_institutions ```


"""

academic_year_colleges.drop(['TUITIONFEE_IN', 'TUITIONFEE_OUT', 'INC_PCT_M2', 'INC_PCT_H1', 'INC_PCT_H2'], axis=1, inplace=True)

"""Veamos una vez más la matriz de correlación de ``` academic_year_institutions ```"""

colormap = plt.cm.viridis
plt.figure(figsize=(10,10))
plt.title('Matriz de correlacion')
sns.heatmap(academic_year_colleges.drop(['STABBR','INSTNM'], axis = 1).astype(float).corr(),
           vmax=1.0,
           vmin=-1,
           cmap=colormap,
           annot=True,
           linewidths=0.1,
           linecolor='white',
           square=True)

plt.show()

"""Distribucion en el espacio de los datos program_year_colleges"""

#Cómo están distribuidos los datos en el espacio
sns.pairplot(program_year_colleges.dropna(),
            height=4, 
            vars=['CONTROL', 'HCM2', 'TUITIONFEE_PROG',	'COSTT4_P',	'INC_PCT_LO','INC_PCT_M1', 'INC_PCT_H2'],
            kind='scatter')
plt.show()

"""Distribucion en el espacio de los datos academic_year_colleges"""

#Cómo están distribuidos los datos en el espacio
sns.pairplot(academic_year_colleges.dropna(),
            height=4, 
            vars=['CONTROL',	'HCM2',	'COSTT4_A',	'INC_PCT_LO','INC_PCT_M1'],
            kind='scatter')
plt.show()

"""Cambio en el nombre de las variables para mas facil entendimiento"""



"""## 3. Cantidad Optima de clusters

### Solucion A

#### Dendograma
"""

#Datos para el modelo
collegesA = collegesA.dropna()
collegesA.shape

#Note que ya no son 1000 registros.

"""Dado que las variables categoricas pueden generar problemas en el modelos hacemos pruebas para ver como estas los afectan"""

import scipy.cluster.hierarchy as shc
from matplotlib import pyplot
pyplot.figure(figsize=(15, 7))  
pyplot.title("Dendrograma SIN Tipo de Monitoreo y Control") 
collegesA_prueba = collegesA.drop(['TIPO_MONITOREO','CONTROL', 'NOMBRE'],axis=1) 
dend = shc.dendrogram(shc.linkage(collegesA_prueba, method='ward'),truncate_mode='level',p=3)

import scipy.cluster.hierarchy as shc
from matplotlib import pyplot
pyplot.figure(figsize=(15, 7))  
pyplot.title("Dendrograma CON Tipo de Monitoreo y Control") 
pyplot.axhline(y=200000)
pyplot.axhline(y=350000)
pyplot.axhline(y=600000)
dend = shc.dendrogram(shc.linkage(collegesA.drop('NOMBRE', axis=1), method='ward'),truncate_mode='level',p=3)

"""Se observa a través de los dendrogramas lo siguiente.
1. La variables *Tipo de Monitoreo y Control* no afecta la forma de agrupación de los datos.
2. Entre 2, 3 y 4 clusteres se obserta un número óptimo por medio de este gráfico.

#### Elbow Curve

`inertia_`: Suma de las distancias cuadráticas de las muestras al centro del cluster mas cercano.

Este método usa la inercia como una medida de la variación intra-cluster e intenta minimizarla. Se debe tener en cuenta que la inercia óptima (mínima) sería cero, para el caso donde cada observación es su propio cluster; pero como esto no es de mucha utilidad, el método del codo escoge como óptimo aquel valor del número de clusters a partir del cual añadir mas clusters solo consigue una mejora mínima de la inercia.
"""

colleges_3D = np.array(collegesA.drop('NOMBRE', axis=1))
#[['attack','defense', 'speed']]
colleges_labels = np.array(college['OPEID'])
print(colleges_3D.shape)

# se define la cantidad de clusters con los que se quiere probar
maxClusters = 20

def elbow_curve(data, maxClusters = 15):

  # rango de valores del parámetro a optimizar (cantidad de clusters)
  maxClusters = range(1, maxClusters + 1)
  inertias = []

  # se ejecuta el modelo para el rango de clusters y se guarda la inercia
  # respectiva obtenida para cada valor
  for k in maxClusters:
    kmeanModel = KMeans(n_clusters = k)
    kmeanModel.fit(colleges_3D)
    inertias.append(kmeanModel.inertia_)

  # Grafico de los resultados obtenidos para cada valor del rango
  print("Valores: ",inertias)
  plt.figure(figsize=(10, 8))
  plt.plot(maxClusters, inertias, 'bx-')
  plt.xlabel('k')
  plt.ylabel('Inertia')
  plt.title('The Elbow Method showing the optimal k')
  plt.show()

elbow_curve(colleges_3D, maxClusters = 10)

"""Vemos que la forma del gráfico nos sugiere que el número adecuado de grupos es de 3 o 4 como máximo.

#### Estadístico de Gap
El objetivo de este método es definir un procedimiento estadístico para formalizar la heurística de la curva de codo. De forma muy simplificada, el estadístico de gap compara, para diferentes valores de k, la variación total intracluster observada frente al valor esperado acorde a una distribución uniforme de referencia (datos de referencia).

El valor a elegir será el k mas pequeño tal que en k+1 el gráfico caiga (no necesariamente es el máximo absoluto de la curva de gap).
"""

# nrefs es la cantidad de datos ("datasets") de referencia contra los que se va a comparar
def optimalK(data, nrefs=3, maxClusters=15):

    gaps = np.zeros((len(range(1, maxClusters+1)),))
    resultsdf = pd.DataFrame({'clusterCount':[], 'gap':[]})
    for gap_index, k in enumerate(range(1, maxClusters+1)):

        # guardara los resultados de dispersión de cada distribución simulada
        refDisps = np.zeros(nrefs)

        # Genera las muestras aleatorias indicadas con nrefs y ejecuta k-means
        # en cada bucle obteniendo los resultados de dispersión (inercia)
        # para cada conjunto generado.
        for i in range(nrefs):
            
            # Crea nuevo conjunto aleatorio de referencia
            # Se puede usar una semilla para tener reproducibilidad
            np.random.seed(0)
            randomReference = np.random.random_sample(size=data.shape)
            
            # se ajusta el modelo al conjunto de referencia
            km = KMeans(k)
            km.fit(randomReference)
            # se guarda la dispersión obtenida
            refDisp = km.inertia_
            refDisps[i] = refDisp

        # Se ajusta el modelo a los datos originales y se guarda su inercia
        km = KMeans(k)
        km.fit(data)
        
        origDisp = km.inertia_

        # Calcula el estadístico de gap para k clusters usando el promedio de
        # las dispersiones de los datos simulados y la dispersión de los datos originales.
        gap = np.log(np.mean(refDisps)) - np.log(origDisp)

        # Guarda el estadístico de gap obtenido en este bucle.
        gaps[gap_index] = gap
        
        resultsdf = resultsdf.append({'clusterCount':k, 'gap':gap}, ignore_index=True)

    # Selecciona el "primer máximo" de los estadísticos obtenidos y devuelve 
    # su respectivo número de clusters    
    for i in range(0, len(gaps)-1):
      if gaps[i+1] <= gaps[i]:
        return (i+1, resultsdf)
    return (len(gaps), resultsdf)
    #return (gaps.argmax() + 1, resultsdf)  # Plus 1 because index of 0 means 1 cluster is optimal, index 2 = 3 clusters are optimal

k, gapdf = optimalK(colleges_3D, nrefs=5, maxClusters=10)
print('La cantidad óptima de clusters es: ', k)

plt.figure(figsize=(16,8))
plt.plot(gapdf['clusterCount'], gapdf['gap'], linestyle='--', marker='o', color='b');
plt.xlabel('K');
plt.ylabel('Gap Statistic');
plt.title('Gap Statistic vs. K');

"""#### Coeficiente de Silueta"""

#Haciendo prueba las variable TIPO DE MONITOREO generan problemas para este analisis por eso los eliminamos 
colleges_3D = collegesA.drop(['TIPO_MONITOREO','CONTROL'], axis =1).copy()
#'CONTROL', 'ESTADO'

colleges_3DA = np.array(colleges_3D.drop('NOMBRE', axis=1))

colleges_labels = np.array(college['OPEID'])
print(colleges_3D.shape)

from sklearn.metrics import silhouette_samples, silhouette_score
import matplotlib.cm as cm

colors_k_means = ['cyan','purple','orange']
range_n_clusters = [2, 3, 4, 5, 6]
X = colleges_3DA

for n_clusters in range_n_clusters:
    # Create a subplot with 1 row and 2 columns
    fig, (ax1, ax2) = plt.subplots(1, 2)
    fig.set_size_inches(19, 4)

    # The 1st subplot is the silhouette plot
    # The silhouette coefficient can range from -1, 1 but in this example all
    # lie within [-0.1, 1]
    ax1.set_xlim([-0.1, 1])
    # The (n_clusters+1)*10 is for inserting blank space between silhouette
    # plots of individual clusters, to demarcate them clearly.
    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])

    # Initialize the clusterer with n_clusters value and a random generator
    # seed of 10 for reproducibility.
    clusterer = KMeans(n_clusters=n_clusters, random_state=10)
    cluster_labels = clusterer.fit_predict(X)

    # The silhouette_score gives the average value for all the samples.
    # This gives a perspective into the density and separation of the formed
    # clusters
    silhouette_avg = silhouette_score(X, cluster_labels)
    print("For n_clusters =", n_clusters,
          "The average silhouette_score is :", silhouette_avg)

    # Compute the silhouette scores for each sample
    sample_silhouette_values = silhouette_samples(X, cluster_labels)

    y_lower = 10
    for i in range(n_clusters):
        # Aggregate the silhouette scores for samples belonging to
        # cluster i, and sort them
        ith_cluster_silhouette_values = \
            sample_silhouette_values[cluster_labels == i]

        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = plt.cm.nipy_spectral(float(i) / n_clusters)
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)

        # Label the silhouette plots with their cluster numbers at the middle
        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

        # Compute the new y_lower for next plot
        y_lower = y_upper + 10  # 10 for the 0 samples

    ax1.set_title("The silhouette plot for the various clusters.")
    ax1.set_xlabel("The silhouette coefficient values")
    ax1.set_ylabel("Cluster label")

    # The vertical line for average silhouette score of all the values
    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

    ax1.set_yticks([])  # Clear the yaxis labels / ticks
    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

    # 2nd Plot showing the actual clusters formed
    colors = plt.cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)
    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,
                c=colors, edgecolor='k')

    # Labeling the clusters
    centers = clusterer.cluster_centers_
    # Draw white circles at cluster centers
    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',
                c="white", alpha=1, s=200, edgecolor='k')

    for i, c in enumerate(centers):
        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,
                    s=50, edgecolor='k')

    ax2.set_title("The visualization of the clustered data.")
    ax2.set_xlabel("Feature space for the 1st feature")
    ax2.set_ylabel("Feature space for the 2nd feature")

    plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
                  "with n_clusters = %d" % n_clusters),
                 fontsize=14, fontweight='bold')

plt.show()

"""Del los cuatro métodos utilizados vemos que dos nos sugieren utilizar 3 grupos. De esta manera consideramos que este es el número óptimo de grupos y haremos el análisis con este número.

### Solucion B
"""



"""## 4. Custering - Jerarquico

### Solucion A

Ahora vamos a implementar un modelo de clustering sobre los datos que tenemos y vamos a caracterizar cada uno de los grupos que nos arroje este.
"""

colleges_3D = collegesA.copy()
colleges_3D.info()

#Normalizar, por buenas practicas, antes de aplicar el modelo.
def minmax_norm(df):
    return (df - df.min()) / ( df.max() - df.min())

#Para hacer el clustering usaremos solo las variables 'COSTO_MATRICULA','INGRESOS_FAMILIARES_M1', 'INGRESOS_FAMILIARES_H2'
collegesA_Norma = colleges_3D.drop(['NOMBRE', 'TIPO_MONITOREO','ESTADO', 'CONTROL'], axis =1).copy()       #Copia del dataframe para no alterarlo, Usamos colleges 3D que no contiene a las variables 
                                            #categoricas, CONTROL y TIPO DE MONITOREO, para evitar posibles errores.
                                            
collegesA_Norma = minmax_norm(collegesA_Norma)
collegesA_Norma.head()

"""#### Caracterizacón   
Diferencia entre _Caracterización_ y _Tendencia_.

* Encontrar una _Tendencia_ en los datos es, explicar de manera **general**, como se comportan los datos al aplicar un tecnica específica cluster. Se buscan realizar afirmaciones independientemente el numero de clusters.

* Encontrar las _Características_ de un cluster, es expresar cuáles son las propiedades representativas de **cada cluster** creado.
"""

#Crear el modelo con la metrica euclidiana y vinculación minimizando la distancia.
# affinity=" " para otros criterios de similitud diferentes al euclidiano que viene por defecto
model=AgglomerativeClustering(n_clusters=3, linkage='ward')

#Aplicar el modelo
data_fit_3 = model.fit(collegesA_Norma)
lab_3c = data_fit_3.labels_

#Se asignará cada label a cada fila correspondiente
#crimen_df_Data_Copy['Labels_3Clusters']=lab_3c     #No se deben realizar cambios a los datos asignados para el modelo
colleges_3D['Labels_3Clusters']=lab_3c
colleges_3D.tail()
#Note que no se le aplicó 'Labels_3Cluster' a crimen_df_Data_Copy (Por ser un dataframe normalizado).
#crimen_df_Data.tail()

#Se separan por labels por medio de una máscara.
#Grupo 0
print("GRUPO 0:")
is_G=colleges_3D.loc[:, 'Labels_3Clusters']==0
C3_G=colleges_3D[is_G]
print(C3_G.head())
print(C3_G.shape)

sns.pairplot(C3_G,
            height=3, 
            vars=['COSTO_MATRICULA','INGRESOS_FAMILIARES_M1', 'INGRESOS_FAMILIARES_H2'],
            kind='scatter')
plt.show()

#Se separan por labels por medio de una máscara.
#Grupo 0
print("GRUPO 1:")
is_G=colleges_3D.loc[:, 'Labels_3Clusters']==1
C3_G=colleges_3D[is_G]
print(C3_G.head())
print(C3_G.shape)

sns.pairplot(C3_G,
            height=3, 
            vars=['COSTO_MATRICULA',	'CONTROL','INGRESOS_FAMILIARES_M1', 'INGRESOS_FAMILIARES_H2'],
            kind='scatter')
plt.show()

#Se separan por labels por medio de una máscara.
#Grupo 2
print("GRUPO 2:")
is_G=colleges_3D.loc[:, 'Labels_3Clusters']==2
C3_G=colleges_3D[is_G]
print(C3_G.head())
print(C3_G.shape)

sns.pairplot(C3_G,
            height=3, 
            vars=['COSTO_MATRICULA',	'CONTROL','INGRESOS_FAMILIARES_M1', 'INGRESOS_FAMILIARES_H2'],
            kind='scatter')
plt.show()

#¿Como visualizar los clusters?
X=np.array(colleges_3D.drop(['NOMBRE', 'CONTROL','ESTADO','TIPO_MONITOREO'],axis=1))
fig = plt.figure(figsize=(10, 8))
ax = Axes3D(fig)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=lab_3c, s=200,
           cmap=matplotlib.colors.ListedColormap(['cyan','purple','orange']), alpha=0.5)
plt.show()

#Otra forma de visualizar los datos en 3D
#Cómo están distribuidos los datos en el espacio
fig = px.scatter_3d(colleges_3D.dropna(), x="INGRESOS_FAMILIARES_M1", y="INGRESOS_FAMILIARES_H2", z="COSTO_MATRICULA",color="Labels_3Clusters")
fig.show()

#Otra forma de visualizar los datos en 3D
#Cómo están distribuidos los datos en el espacio
fig = px.scatter_3d(colleges_3D.dropna(), x="CONTROL", y="INGRESOS_FAMILIARES_M1", z="COSTO_MATRICULA",color="Labels_3Clusters")
fig.show()

#Otra forma de visualizar los datos en 3D
#Cómo están distribuidos los datos en el espacio
fig = px.scatter_3d(colleges_3D.dropna(), x="CONTROL", y="INGRESOS_FAMILIARES_H2", z="COSTO_MATRICULA",color="Labels_3Clusters")
fig.show()

#Otra forma de visualizar los datos en 3D
#Cómo están distribuidos los datos en el espacio
fig = px.scatter_3d(colleges_3D.dropna(), x="INGRESOS_FAMILIARES_M1", y="INGRESOS_FAMILIARES_H2", z="COSTO_MATRICULA",color="CONTROL")
fig.show()

"""##Caracterización de los grupos

Veamos un poco más sobre el comportamiento de los grupos. Hagamos un pequeño análisis descriptivo sobre cada grupo.
"""

grupo_0 = colleges_3D[colleges_3D.Labels_3Clusters == 0]
grupo_0.head()

"""Vamos a darle formato de string a las variables categóricas para garantizar un análisis más limpio"""

grupo_0 = grupo_0.astype({'CONTROL': 'object', 'TIPO_MONITOREO': 'object', 'ESTADO': 'object' })
grupo_0.info()

"""Veamos algunas estadísticas básicas generales"""

grupo_0.describe()

"""Hagamos lo mismo con los grupos 1 y 2"""

grupo_1 = colleges_3D[colleges_3D.Labels_3Clusters ==1]
grupo_1 = grupo_1.astype({'CONTROL': 'object', 'TIPO_MONITOREO': 'object', 'ESTADO': 'object' })
grupo_1.describe()

grupo_2 = colleges_3D[colleges_3D.Labels_3Clusters ==2]
grupo_2 = grupo_2.astype({'CONTROL': 'object', 'TIPO_MONITOREO': 'object', 'ESTADO': 'object' })
grupo_2.describe()

"""Algo destacable de estas tres tablas es la media y la mediana. Tal como vemos, ambas son más altas para el grupo 1 y más bajas para el grupo 2, mientras que para el grupo 0 están en un punto intermedio. Esto nos sugiere una caracterización como la siguiente:

####Grupo 0: Universidades de costo medio.
####Grupo 1: Universidades de costo alto.
####Grupo 2: Universidades de costo bajo.

Pese a esto, notemos que la universidad de mayor costo no está en el grupo 1 sino en el grupo 0. Aún así, la desviación estandar y el rango intercuartil del grupo 0 son relativamente bajos, lo que sugiere que la distribución del costo en este grupo no está muy dispersa y el máximo es solo un valor atípico; veamos exactamente qué universidad es.
"""

grupo_0[grupo_0.COSTO_MATRICULA == grupo_0.COSTO_MATRICULA.max()]

"""En particular esta es una universidad enfocada en culinaria; deberíamos investigar más a fondo el motivo por el cual el modelo la agrupó con universidades de costo medio en general, pero eso queda por fuera del alcance del proyecto.

Notemos que la desviación estandar y el rango intercuartil del costo en los grupos 0 y 2 son muy similares, a diferencia del grupo 1 en el cual la diferencia es de casi $8000, lo cual nos indica que hay universidades de costo medio también en el grupo 3. Esto se puede explicar dándole un vistazo también a las variables de ingresos familiares; tal como vemos, en particular en el grupo 1 vemos porcentajes altos en  ```INGRESOS_FAMILIARES_H2```, este campo se refiere a porcentaje de estudiantes cuyas familias tienen ingresos altos, lo que significa que aunque el costo de algunas universidades de este grupo puede ser considerado como un costo medio, el porcentaje de estudiantes de familias de clase alta en estas es mayor que el promedio en los grupos 0 y 2. De esta manera optamos por darles a los grupos la siguiente caracterización:


####**Grupo 0: universides de costo medio y con estudiantes principalmente de clase media**
####**Grupo 1: Universidades de costo alto y estudiantes principalmente de clase alta**
####**Grupo 2: Universidades de costo bajo y estudiantes principalmente de clase media**

Finalmente descargamos los datos resultantes
"""

from google.colab import files

colleges_3D.to_csv('colleges_agrupados.csv') 
grupo_0.to_csv('grupo_0.csv')
grupo_1.to_csv('grupo_1.csv')
grupo_2.to_csv('grupo_2.csv')

files.download('colleges_agrupados.csv')
files.download('grupo_1.csv')
files.download('grupo_2.csv')
files.download('grupo_0.csv')

"""### Solucion B"""

